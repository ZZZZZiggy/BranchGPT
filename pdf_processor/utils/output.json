[
  {
    "chapter": "CUBIC: A New TCP-Friendly High-Speed TCP Variant",
    "page": 1,
    "content": "∗"
  },
  {
    "chapter": "Sangtae Ha, Injong Rhee",
    "page": 1,
    "content": "Raleigh, NC 27695"
  },
  {
    "chapter": "{sha2,rhee}@ncsu.edu",
    "page": 1,
    "content": "{sha2,rhee}@ncsu.edu"
  },
  {
    "chapter": "Lisong Xu",
    "page": 1,
    "content": "xu@cse.unl.edu"
  },
  {
    "chapter": "ABSTRACT",
    "page": 1,
    "content": "CUBIC is a congestion control protocol for TCP (transmis-\nsion control protocol) and the current default TCP algo-\nrithm in Linux. The protocol modiﬁes the linear window\ngrowth function of existing TCP standards to be a cubic\nfunction in order to improve the scalability of TCP over\nfast and long distance networks. It also achieves more eq-\nuitable bandwidth allocations among ﬂows with diﬀerent\nRTTs (round trip times) by making the window growth to\nbe independent of RTT – thus those ﬂows grow their conges-\ntion window at the same rate. During steady state, CUBIC\nincreases the window size aggressively when the window is\nfar from the saturation point, and the slowly when it is close\nto the saturation point. This feature allows CUBIC to be\nvery scalable when the bandwidth and delay product of the\nnetwork is large, and at the same time, be highly stable and\nalso fair to standard TCP ﬂows.\nThe implementation of\nCUBIC in Linux has gone through several upgrades. This\npaper documents its design, implementation, performance\nand evolution as the default TCP algorithm of Linux."
  },
  {
    "chapter": "1.INTRODUCTION",
    "page": 1,
    "content": "As the Internet evolves to include many very high speed\nand long distance network paths, the performance of TCP\nwas challenged. These networks are characterized by large\nbandwidth and delay product (BDP) which represents the\ntotal number of packets needed in ﬂight while keeping the\nbandwidth fully utilized, in other words, the size of the con-\ngestion window.\nIn standard TCP like TCP-Reno, TCP-\nNewReno and TCP-SACK, TCP grows its window one per\nround trip time (RTT). This makes the data transport speed\nof TCP∗used in all major operating systems including Win-\ndows and Linux rather sluggish, to say the least, extremely\nunder-utilizing the networks especially if the length of ﬂows\nis much shorter than the time TCP grows its windows to\nthe full size of the BDP of a path. For instance, if the band-\nwidth of a network path is 10 Gbps and the RTT is 100 ms,\nwith packets of 1250 bytes, the BDP of the path is around\n100,000 packets. For TCP to grow its window from the mid-\npoint of the BDP, say 50,000, it takes about 50,000 RTTs\nwhich amounts to 5000 seconds (1.4 hours). If a ﬂow ﬁnishes\nbefore that time, it severely under-utilizes the path.\nTo counter this under-utilization problem of TCP, many\n∗A short version [27] of this paper was presented at the Inter-\nnational Workshop on Protocols for Fast and Long Distance\nNetworks in 2005.\n∗For brevity, we also denote Standard TCP as TCP.\n“high-speed” TCP variants are proposed (e.g., FAST [24],\nHSTCP [15], STCP [25], HTCP [28], SQRT [19], West-\nwood [14], and BIC-TCP [30]). Recognizing this problem\nwith TCP, the Linux community responded quickly to im-\nplement a majority of these protocols in Linux and ship\nthem as part of its operating system. After a series of third-\nparty testing and performance validation [11, 21], in 2004,\nfrom version 2.6.8, it selected BIC-TCP as the default TCP\nalgorithm and the other TCP variants as optional.\nWhat makes BIC-TCP stand out from other TCP algor-\ntihms is its stability. It uses a binary search algorithm where\nthe window grows to the mid-point between the last win-\ndow size (i.e., max) where TCP has a packet loss and the\nlast window size (i.e., min) it does not have a loss for one\nRTT period. This “search” into the mid-point intuitively\nmakes sense because the capacity of the current path must\nbe somewhere between the two min and max window sizes\nif the network conditions do not quickly change since the\nlast congestion signal (which is the last packet loss). After\nthe window grows to the mid-point, if the network does not\nhave packet losses, then it means that the network can han-\ndle more traﬃc and thus BIC-TCP sets the mid-point to\nbe the new min and performs another “binary-search” with\nthe min and max windows. This has an eﬀect of growing\nthe window really fast when the current window size is far\nfrom the available capacity of the path, and furthermore, if\nit is close to the available capacity (where we had the pre-\nvious loss), it slowly reduces its window increment. It has\nthe smallest window increment at the saturation point and\nits overshoots amount beyond the saturation point where\nlosses occur very small.\nThe whole window growth func-\ntion is simply a logarithmic concave function. This concave\nfunction keeps the congestion window much longer at the\nsaturation point or equilibrium than convex or linear func-\ntions where they have the largest window increment at the\nsaturation point and thus have the largest overshoot at the\ntime packet losses occur.\nThese features make BIC-TCP\nvery stable and at the same time highly scalable.\nBIC-TCP trades the speed to react to changes in avail-\nable bandwidth (i.e., convergence speed) for stability. If the\navailable capacity has increased since the last packet losses,\nthe window can grow beyond the max without having a loss.\nAt that time, BIC-TCP increases the window exponentially.\nNote that an exponential function (a convex function) grows\nvery slowly at the beginning (slower than a linear function).\nThis feature adds to the stability of the protocol because\neven if the protocol makes mistakes in ﬁnding the max win-\ndow, it ﬁnds the next max window near the previous max\npoint ﬁrst, thus staying at the previous saturation point\nlonger. But the exponential function quickly catches up and\nits increment becomes very large if the losses do not occur\n(in which case, the saturation point has become much larger\nthan the previous one). Because it stays longer near the pre-\nvious saturation point than other variants, it can be slug-\ngish to ﬁnd the new saturation point if the saturation point\nhas increased far beyond the last one. BIC-TCP, however,\nsafely reacts fast to reduced capacity because packet losses\noccur before the previous max and it reduces the window\nby a multiplicative factor. This tradeoﬀis a design choice\nof BIC-TCP. It is known [31] that available bandwidth in\nthe Internet change over a long time scale of several hours.\nGiven that packet losses would occur very asynchronously\nand also proportionally to the bandwidth consumption of a\nﬂow under a highly statistically multiplexed environment,\nfast convergence is a natural consequence of the network en-\nvironment – something the protocol does not have to force.\nThus, although BIC-TCP may converge slowly under low\nstatistical multiplexing where only a few ﬂows are compet-\ning, its convergence speed is not an issue under typical In-\nternet environments.\nCUBIC [27] is the next version of BIC-TCP. It greatly sim-\npliﬁes the window adjustment algorithm of BIC-TCP by re-\nplacing the concave and convex window growth portions of\nBIC-TCP by a cubic function (which contains both concave\nand convex portions).\nIn fact, any odd order polynomial\nfunction has this shape. The choice for a cubic function is\nincidental and out of convenience. The key feature of CU-\nBIC is that its window growth depends only on the real time\nbetween two consecutive congestion events. One congestion\nevent is the time when TCP undergoes fast recovery. We call\nthis real time a congestion epoch. Thus, the window growth\nbecomes independent of RTTs. This feature allows CUBIC\nﬂows competing in the same bottleneck to have approxi-\nmately the same window size independent of their RTTs,\nachieving good RTT-fairness. Furthermore, when RTTs are\nshort, since the window growth rate is ﬁxed, its growth rate\ncould be slower than TCP standards. Since TCP standards\n(e.g., TCP-SACK) work well under short RTTs, this feature\nenhances the TCP-friendliness of the protocol.\nThe implementation of CUBIC in Linux has gone through\nseveral upgrades. The most notable upgrade is the eﬃcient\nimplementation of cubic root calculation. Since it requires\na ﬂoating point operation, implementing it in the kernel re-\nquires some integer approximation. Initially it used the bi-\nsection method and later changed to the Newton-Raphson\nmethod which reduces the computational cost almost by 10\ntimes. Another change to CUBIC after inception is the re-\nmoval of window clamping.\nWindow clamping was intro-\nduced in BIC-TCP where window increments are clamped\nto a maximum increment and was inherited to CUBIC for\nthe ﬁrst version. This forces the window growth to be linear\nwhen the target mid-point is much larger than the current\nwindow size. The authors conclude that this feature is not\nneeded after extensive testing due to the increased stability\nof CUBIC. CUBIC replaced BIC-TCP as the default TCP\nalgorithm in 2006 after version 2.6.18.\nThe changes and\nupgrades of CUBIC in Linux are documented in Table 1.\nThe remainder of this paper is organized as follows. Section\n2 gives related work, Section 3 presents the details of CU-\nBIC algorithms in Linux, Section 4 includes the evolution\nof CUBIC and its implementation in Linux, and Section 5\nincludes discussion related to fairness property of CUBIC.\nSection 6 presents the results of experimental evaluation and\nSection 7 gives conclusion."
  },
  {
    "chapter": "2.RELATED WORK",
    "page": 2,
    "content": "Kelly proposed Scalable TCP (STCP) [25]. The design ob-\njective of STCP is to make the recovery time from loss events\nbe constant regardless of the window size. This is why it\nis called “Scalable”. Note that the recovery time of TCP-\nNewReno largely depends on the current window size.\nHighSpeed TCP (HSTCP) [15] uses a generalized AIMD\nwhere the linear increase factor and multiplicative decrease\nfactor are adjusted by a convex fucntion of the current con-\ngestion window size. When the congestion window is less\nthan some cutoﬀvalue, HSTCP uses the same factors as\nTCP. Most of high-speed TCP variants support this form\nof TCP compatibility, which is based on the window size.\nWhen the window grows beyond the cutoﬀpoint, the con-\nvex function increases the increase factor and reduces the\ndecrease factor proportionally to the window size.\nHTCP [28], like CUBIC, uses the elapsed time (∆) since\nthe last congestion event for calculating the current conges-\ntion window size. The window growth function of HTCP\nis a quadratic function of ∆.\nHTCP is unique in that it\nadjusts the decrease factor by a function of RTTs which is\nengineered to estimate the queue size in the network path\nof the current ﬂow. Thus, the decrease factor is adjusted to\nbe proportional to the queue size.\nTCP-Vegas [10] measures the diﬀerence (δ) between expected\nthroughput and actual throughput based on round-trip de-\nlays.\nWhen δ is less than a low threshold α, TCP-Vegas\nbelieves the path is not congested and thus increases the\nsending rate. When δ is larger than a upper threshold β,\nwhich is a strong indication of congestion, TCP-Vegas re-\nduces the sending rate. Otherwise, TCP-Vegas maintains\nthe current sending rate. The expected throughput is cal-\nculated by dividing the current congestion window by the\nminimum RTT which typically contains the delay when the\npath is not congested. For each round trip time, TCP-Vegas\ncomputes the actual throughput by dividing the number of\npackets sent by the sampled RTT.\nFAST [24] determines the current congestion window size\nbased on both round-trip delays and packet losses over a\npath. FAST updates the sending rate at every other RTT\nwith rate-pacing. The algorithm estimates the queuing de-\nlay of the path using RTTs and if the delay is well below\na threshold, it increases the window aggressively and if it\ngets closer to the threshold, the algorithm slowly reduces\nthe increasing rate. The opposite happens when the delay\nincreases beyond the threshold: slowly decreases the window\nﬁrst and then aggressively decreases the window. For packet\nlosses, FAST halves the congestion window and enters loss\nrecovery just like TCP.\nTCP-Westwood [14] estimates an end-to-end available band-\nwidth by accounting the rate of returning ACKs. For packet\nlosses, unlike TCP which “blindly” reduces the congestion\nwindow to the half, TCP-Westwood sets the slow start thresh-\nold to this estimate. This mechanism is eﬀective especially\nover wireless links where frequent channel losses are mis-\ninterpreted as congestion losses and thus TCP reduces the\ncongestion window unnecessarily.\nTCP-Illinois [26] uses a queueing delay to determine an in-\ncrease factor α and multiplicative decrease factor β instan-\ntaneously during the window increment phase.\nPrecisely,\nTCP-Illinois sets a large α and small β when the average\ndelay d is small, which is the indication that congestion is\nnot imminent, and sets a small α and large β when d is large\nbecause of imminent congestion.\nTCP-Hybla [13] scales the window increment rule to ensure\nfairness among the ﬂows with diﬀerent RTTs. TCP-Hybla\nbehaves as TCP-NewReno when the RTT of a ﬂow is less\nthan a certain reference RTT (e.g., 20ms). Otherwise, TCP-\nHybla increases the congestion window size more aggres-\nsively to compensate throughput drop due to RTT increase.\nTCP-Veno [17] determines the congestion window size very\nsimilar to TCP-NewReno, but it uses the delay information\nof TCP-Vegas to diﬀerentiate non-congestion losses. When\npacket loss happens, if the queue size inferred by the delay\nincrease is within a certain threshold, which is the strong\nindication of random loss, TCP-Veno reduces the congestion\nwindow by 20%, not by 50%."
  },
  {
    "chapter": "3.CUBIC CONGESTION CONTROL",
    "page": 3,
    "content": ""
  },
  {
    "chapter": "3.1.BIC-TCP",
    "page": 3,
    "content": "In this section, we give some details on BIC-TCP which is a\npredecessor of CUBIC. The main feature of BIC-TCP is its\nunique window growth function as discussed in the introduc-\ntion. Figure 1 shows the growth function of BIC-TCP. When\nit gets a packet loss event, BIC-TCP reduces its window by\na multiplicative factor β. The window size just before the\nreduction is set to the maximum Wmax and the window size\njust after the reduction is set to the minimum Wmin. Then,\nBIC-TCP performs a binary search using these two param-\neters - by jumping to the “midpoint” between Wmax and\nWmin. Since packet losses have occurred at Wmax, the win-\ndow size that the network can currently handle without loss\nmust be somewhere between these two numbers.\nHowever, jumping to the midpoint could be too much in-\ncrease within one RTT, so if the distance between the mid-\npoint and the current minimum is larger than a ﬁxed con-\nstant, called Smax, BIC-TCP increments cwnd by Smax (lin-\near increase). If BIC-TCP does not get packet losses at the\nupdated window size, that window size becomes the new\nminimum. This process continues until the window incre-\nment is less than some small constant called Smin at which\npoint, the window is set to the current maximum. So the\ngrowth function after a window reduction will be most likely\nto be a linear one followed by a logarithmic one (marked as\n“additive increase” and “binary search” respectively in Fig-\nure 1 (a).)\nIf the window grows past the maximum, the equilibrium\nwindow size must be larger than the current maximum and a\n(a) BIC-TCP window growth function.\n(b) CUBIC window growth function.\nand CUBIC.\nnew maximum must be found. BIC-TCP enters a new phase\ncalled “max probing”. Max probing uses a window growth\nfunction exactly symmetric to those used in additive increase\nand binary search (which is logarithmic; its reciprocal will\nbe exponential) and then additive increase.\nFigure 1 (a)\nshows the growth function during max probing. During max\nprobing, the window grows slowly initially to ﬁnd the new\nmaximum nearby, and after some time of slow growth, if it\ndoes not ﬁnd the new maximum (i.e., packet losses), then\nit guesses the new maximum is further away so it switches\nto a faster increase by switching to additive increase where\nthe window size is incremented by a large ﬁxed increment.\nThe good performance of BIC-TCP comes from the slow\nincrease around Wmax and linear increase during additive\nincrease and max probing."
  },
  {
    "chapter": "3.2.CUBIC window growth function",
    "page": 3,
    "content": "BIC-TCP achieves good scalability in high speed networks,\nfairness among competing ﬂows of its own and stability with\nlow window oscillations. However, BIC-TCP’s growth func-\ntion can still be too aggressive for TCP, especially under\nshort RTT or low speed networks. Furthermore, the several\ndiﬀerent phases (binary search increase, max probing, Smax\nand Smin) of window control add complexity in implement-\ning the protocol and analyzing its performance. We have\nbeen searching for a new window growth function that while\nretaining strengths of BIC-TCP (especially, its stability and\nscalability), simpliﬁes the window control and enhances its\nTCP friendliness.\nWe introduce a new high-speed TCP variant: CUBIC. As\nthe name of the protocol represents, the window growth\nfunction of CUBIC is a cubic function whose shape is very\nsimilar to the growth function of BIC-TCP. CUBIC uses a\ncubic function of the elapsed time from the last congestion\nevent. While most alternative algorithms to Standard TCP\nuses a convex increase function where after a loss event, the\nwindow increment is always increasing, CUBIC uses both\nthe concave and convex proﬁles of a cubic function for win-\ndow increase.\nFigure 1 (b) shows the growth function of\nCUBIC.\nThe details of CUBIC are as follows. After a window re-\nduction following a loss event, it registers Wmax to be the\nwindow size where the loss event occurred and performs a\nmultiplicative decrease of congestion window by a factor of\nβ where β is a window decrease constant and the regular fast\nrecovery and retransmit of TCP. After it enters into conges-\ntion avoidance from fast recovery, it starts to increase the\nwindow using the concave proﬁle of the cubic function. The\ncubic function is set to have its plateau at Wmax so the con-\ncave growth continues until the window size becomes Wmax.\nAfter that, the cubic function turns into a convex proﬁle and\nthe convex window growth begins. This style of window ad-\njustment (concave and then convex) improves protocol and\nnetwork stability while maintaining high network utiliza-\ntion [12]. This is because the window size remains almost\nconstant, forming a plateau around Wmax where network\nutilization is deemed highest and under steady state, most\nwindow size samples of CUBIC are close to Wmax, thus pro-\nmoting high network utilization and protocol stability. Note\nthat protocols with convex growth functions tend to have\nthe largest window increment around the saturation point,\nintroducing a large burst of packet losses.\nThe window growth function of CUBIC uses the following\nfunction:\nW (t) = C(t −K)3 + Wmax\n(1)\nwhere C is a CUBIC parameter, t is the elapsed time from\nthe last window reduction, and K is the time period that the\nabove function takes to increase W to Wmax when there is\nno further loss event and is calculated by using the following\nequation:\nr\nWmaxβ\n(2)\nUpon receiving an ACK during congestion avoidance, CU-\nBIC computes the window growth rate during the next RTT\nperiod using Eq. (1). It sets W (t + RTT) as the candidate\ntarget value of congestion window. Suppose that the cur-\nrent window size is cwnd. Depending on the value of cwnd,\nCUBIC runs in three diﬀerent modes. First, if cwnd is less\nthan the window size that (standard) TCP would reach at\ntime t after the last loss event, then CUBIC is in the TCP\nmode (we describe below how to determine this window size\nof standard TCP in term of time t).\nOtherwise, if cwnd\nis less than Wmax, then CUBIC is in the concave region,\nand if cwnd is larger than Wmax, CUBIC is in the convex\nregion. Algorithm 1 shows the pseudo-code of the window\nadjustment algorithm of CUBIC implemented in Linux."
  },
  {
    "chapter": "3.3.TCP-friendly region",
    "page": 4,
    "content": "When receiving an ACK in congestion avoidance, we ﬁrst\ncheck whether the protocol is in the TCP region or not. This\nis done as follows. We can analyze the window size of TCP\nin terms of the elapsed time t. Using a simple analysis in\n[16], we can ﬁnd the average window size of additive increase\nand multiplicative decrease (AIMD) with an additive factor\n........................... (3.7)\nα and a multiplicative factor β to be the following function:\n1\nBy the same analysis, the average window size of TCP with\nα = 1 and β = 0.5 is\n1\np. Thus, for Eq. 3 to be\nthe same as that of TCP, α must be equal to\n3β\n2−β . If TCP\nincreases its window by α per RTT, we can get the window\nsize of TCP in terms of the elapsed time t as follows:\nWtcp(t) = Wmax(1 −β) + 3\nβ"
  },
  {
    "chapter": "3.4.Concave region",
    "page": 5,
    "content": "When receiving an ACK in congestion avoidance, if the pro-\ntocol is not in the TCP mode and cwnd is less than Wmax,\nthen the protocol is in the concave region. In this region,\ncwnd is incremented by W (t+RT T )−cwnd\n, which is shown at\n(3.4) in Algorithm 1."
  },
  {
    "chapter": "3.5.Convex region",
    "page": 5,
    "content": "When the window size of CUBIC is larger than Wmax, it\npasses the plateau of the cubic function after which CU-\nBIC follows the convex proﬁle of the cubic function. Since\ncwnd is larger than the previous saturation point Wmax,\nthis indicates that the network conditions might have been\nperturbed since the last loss event, possibly implying more\navailable bandwidth after some ﬂow departures. Since the\nInternet is highly asynchronous, ﬂuctuations in available\nbandwidth always exist. The convex proﬁle ensures that the\nwindow increases very slowly at the beginning and gradu-\nally increases its growth rate. We also call this phase as the\nmaximum probing phase since CUBIC is searching for a new\nWmax. As we do not modify the window increase function\nonly for the convex region, the window growth function for\nboth regions remains unchanged. To be exact, if the pro-\ntocol is the convex region outside the TCP mode, cwnd is\nincremented by W (t+RT T )−cwnd\n, which is shown at (3.5) in\nAlgorithm 1."
  },
  {
    "chapter": "3.6.Multiplicative decrease",
    "page": 5,
    "content": "When a packet loss occurs, CUBIC reduces its window size\nby a factor of β. We set β to 0.2. A side eﬀect of setting β to\na smaller value than 0.5 is slower convergence. We believe\nthat while a more adaptive setting of β could result in faster\nconvergence, it will make the analysis of the protocol much\nharder and also aﬀects the stability of the protocol. This\nadaptive adjustment of β is a future research issue."
  },
  {
    "chapter": "3.7.Fast Convergence",
    "page": 5,
    "content": "To improve the convergence speed of CUBIC, we add a\nheuristic in the protocol. When a new ﬂow joins the net-\nwork, existing ﬂows in the network need to give up their\nbandwidth shares to allow the new ﬂow some room for growth.\nTo increase this release of bandwidth by existing ﬂows, we\nadd the following mechanism called fast convergence.\nWith fast convergence, when a loss event occurs, before a\nwindow reduction of the congestion window, the protocol\nremembers the last value of Wmax before it updates Wmax\nfor the current loss event. Let us call the last value of Wmax\nto be Wlast\nAt a loss event, if the current value of\nWmax is less than the last value of it, Wlast\nmax, this in-\ndicates that the saturation point experienced by this ﬂow\nis getting reduced because of the change in available band-\nwidth. Then we allow this ﬂow to release more bandwidth\nby reducing Wmax further. This action eﬀectively lengthens\nthe time for this ﬂow to increase its window because the re-\nduced Wmax forces the ﬂow to have the plateau earlier. This\nallows more time for the new ﬂow to catch up its window\nsize. The pseudo code for this operation is shown at (3.7)\nin Algorithm 1."
  },
  {
    "chapter": "4.CUBIC IN LINUX KERNEL",
    "page": 5,
    "content": "Since the ﬁrst release of CUBIC to the Linux community\nin 2006, CUBIC has gone through several upgrades. This\nsection documents those changes."
  },
  {
    "chapter": "4.1.Evolution of CUBIC in Linux",
    "page": 5,
    "content": "Table 1 summarizes important updates [1] on the implemen-\ntation of CUBIC in Linux since its ﬁrst introduction in Linux\n2.6.13. The most updates on CUBIC are focussed on per-\nformance and implementation eﬃciency improvements. One\nof notable optimizations is the improvement on cubic root\ncalculation. The implementation of CUBIC requires solving\nEq. 2, a cubic root calculation. The initial implementation\nof CUBIC\n[18] in Linux uses the bisection method. But\nthe Linux developer community worked together to replace\nit with the Newton-Rhaphson method which improves the\nrunning time by more than 10 times on average (1032 clocks\nvs. 79 clocks) and reduces the variance in running times.\nCUBIC also went through several algorithmic changes to\nhave its current form to enhance its scalability, fairness and\nconvergence speed."
  },
  {
    "chapter": "4.2.Pluggable Congestion Module",
    "page": 5,
    "content": "More inclusions of TCP variants to the Linux kernel has\nsubstantially increased the complexity of the TCP code in\nthe kernel. Even though a new TCP algorithm comes with a\npatch for the kernel, this process requires frequent kernel re-\ncompilations and exacerbates the stability of the TCP code.\nTo eliminate the need of kernel recompilation and help ex-\nperimenting with a new TCP algorithm with Linux, Stephen\nHemminger introduces a new architecture [23, 6], called\npluggable congestion module, in Linux 2.6.13. It is dynami-\ncally loadable and allows switching between diﬀerent conges-\ntion control algorithm modules on the ﬂy without recompi-\nlation. Figure 2 shows the interface to this module, named\ntcp\na hook in the TCP code that provides access to the TCP\ncode. A new congestion control algorithm requires to deﬁne\ncong\navoid and ssthresh, but the other methods are optional.\nThe init and release functions are called for the initializa-\ntion and termination of a given TCP algorithm. ssthresh is\nthe slow start threshold which is called when the given TCP\ndetects a loss. The lower bound on congestion window is\nthe slow start threshold, but when congestion control needs\nto override this lower bound, min\ncwnd can be used for that\navoid is called whenever an ACK arrives and\nthe congestion window (cwnd) is adjusted. For instance, in\nstandard TCP New-Reno, when an ACK arrives, cong\ncwnd (during congestion avoidance). set\nis called when the congestion control state of TCP is changed\namong Normal, Loss Recovery, Loss Recovery after Time-\nout, Reordering, and Congestion Window Reduction. cwnd\nis called when the events deﬁned in tcp\nca\nevent occur. When\nan algorithm requires to handle one of the events, it can\ncreate a hook to cwnd\nevent which is called when the cor-\nresponding event occurs. undo\ncwnd handles false detection\nof loss or timeout. When TCP realizes the change to cwnd\nis wrong, it falls back to the original cwnd using undo\npkts\nacked is a hook for counting ACKs; many protocols\n(e.g., BIC-TCP, CUBIC, and H-TCP) also use this hook\nto get RTT information.\nget\ninfo is a hook for providing\ncongestion control information to the user space.\nCUBIC has been implemented as one of pluggable conges-\ntion control modules.\nThe followings are the hooks that\nCUBIC use for its implementation [3].\n1. bictcp\ninit: initializes private variables used for CU-\nBIC algorithm.\nIf initial\nssthresh is not 0, then set\nssthresh to this value. If initial\nssthresh is properly set\nby users when there is no history information about\nthe end-to-end path, it can improve the start-up be-\nhavior of CUBIC signiﬁcantly.\n2. bictcp\nrecalc\nIf the fast convergence mode\nis turned on and the current cwnd is smaller than\nlast\navoid: increases cwnd by computing the\ndiﬀerence between the current cwnd value and its ex-\npected value of the next RTT round which is obtained\nby cubic root calculation.\n4. bictcp\nset\nstate: resets all the variables when a timeout\nhappens.\n5. bictcp\nundo\ncwnd: returns the maximum between the\ncurrent cwnd value and the last\nmax (which is the con-\ngestion window before the drop).\n6. bictcp\nacked: maintains the minimum delay observed\nso far. The minimum delay is reset when a timeout\nhappens."
  },
  {
    "chapter": "5.DISCUSSION",
    "page": 6,
    "content": "With a deterministic loss model where the number of packets\nbetween two successive loss events is always\n1\np, CUBIC al-\nways operates with the concave window proﬁle which greatly\nsimpliﬁes the performance analysis of CUBIC. The average\nwindow size of CUBIC can be obtained by the following\nfunction:\ns\nTo ensure fairness to Standard TCP based on our argument\nin the introduction, we set C to 0.4.\nWe ﬁnd that this\nvalue of C allows the size of the TCP friendly region to be\nlarge enough to encompass most of the environments where\nStandard TCP performs well while preserving the scalability\nof the window growth function. With β set to 0.2, the above\nformula is reduced to the following function:\nr\n(RTT\n)3\n(6)\n(6) is used to argue the fairness of CUBIC to Standard TCP\nand its safety for deployment below."
  },
  {
    "chapter": "5.1.Fairness to standard TCP",
    "page": 6,
    "content": "In environments where standard TCP is able to make rea-\nsonable use of the available bandwidth, CUBIC does not\nsigniﬁcantly change this state.\nStandard TCP performs well in the following two types of\nnetworks:\n1. networks with a small bandwidth-delay product (BDP).\n2. networks with a short RTT, but not necessarily a small\nBDP.\nCUBIC is designed to behave very similarly to standard\nTCP in the above two types of networks. Figure 3 shows the\nresponse function (average window size) of standard TCP,\nHSTCP, and CUBIC. The average window size of standard\nTCP and HSTCP is from [15]. The average window size of\nCUBIC is calculated by using (6) and CUBIC TCP-friendly\nequation in (4). Figure 3 shows that CUBIC is more friendly\nto TCP than HSTCP, especially in networks with a short\nRTT where TCP performs reasonably well. For example,\nin a network with RTT = 10ms and p = 10−6, TCP has\nan average window of 1200 packets.\nIf the packet size is\n1500 bytes, then TCP can achieve an average rate of 1.44\nGbps. In this case, CUBIC achieves exactly the same rate\nas Standard TCP, whereas HSTCP is about ten times more\naggressive than Standard TCP.\n10\n100\n1000\n10000\n100000\n1e+06\n0.1\nAvg. Window Size (packets)\nRandom Loss Rate (packets)\nStandard TCP\nHSTCP\nCUBIC\n(a) Networks with 10ms RTT.\n10\n100\n1000\n10000\n100000\n1e+06\n0.1\nAvg. Window Size (packets)\nRandom Loss Rate (packets)\nStandard TCP\nHSTCP\nCUBIC\n(b) Networks with 100ms RTT.\n100ms (b) RTTs respectively."
  },
  {
    "chapter": "5.2.CUBIC in action",
    "page": 7,
    "content": "Figure 4 shows the window curve of CUBIC over the running\ntime. This graph is obtained by running testbed experiment\non a dumbbell network conﬁguration with signiﬁcant back-\nground traﬃc in both directions. The bottleneck capacity is\n400Mbps and the RTT is set to 240ms. Drop tail routers are\nused. There are two CUBIC ﬂows, and they have the same\nRTT and bottleneck. Note that the curves have plateaus\naround Wmax which is the window size at the time of the\nlast packet loss event. We observe that two ﬂows use all\nphases of CUBIC functions over the running time and two\nﬂows converges to a fair share within 200 seconds.\nFigure 5 shows the friendliness of CUBIC with respect to\nTCP-SACK. In this experiment, we run one CUBIC ﬂow\nwith one TCP-SACK ﬂow over a short-RTT network path\n(8ms) and a long-RTT network path (82ms), respectively.\nUnder the short-RTT (8ms) network where even TCP-SACK\ncan use the full bandwidth of the path, CUBIC operates in\nthe TCP-friendly mode. Figure 5 (a) conﬁrms that one CU-\nBIC ﬂow runs in the TCP-friendly mode and shares the\nbandwidth fair with the other TCP-SACK ﬂow by main-\ntaining the congestion window of CUBIC similar with that\nof TCP-SACK. Under the long-RTT (82ms) network where\nStandard TCP has the under-utilization problem, CUBIC\nuses a cubic function to be scalable for this environment.\nFigure 5 (b) conﬁrms that the CUBIC ﬂow runs a cubic win-\ndow growth function unlike the case with the short-RTT net-\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\n18000\n600\n(packets)\nTime (second)\nFast Convergence\nConcave Growth\nConvex Growth\nMultiplicative DecreaseCUBIC flow1\nCUBIC flow2\n(a) CUBIC window curves.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n600\n(Mbps)\nTime (second)\nCUBIC flow2\n(b) Throughput of two CUBIC ﬂows.\nFigure 4: Two CUBIC ﬂows with 246ms RTT.\nwork where CUBIC is indistinguishable with TCP-SACK.\nFigure 6 shows the experiment with four TCP-SACK ﬂows\nand four CUBIC ﬂows.\nFor this experiment, we set the\nbandwidth to 400Mbps, RTT to 40ms, and buﬀer size to\n100% BDP of a ﬂow. We observe that four ﬂows of CUBIC\nconverge to a fair share nicely within a short period of time.\nTheir cwnd curves are very smooth and do not cause much\ndisturbance to competing TCP ﬂows. In this experiment,\nthe total network utilization is around 95%: the four CUBIC\nﬂows take about 72% of the total bandwidth, the four TCP\nﬂows take 23%."
  },
  {
    "chapter": "6.EXPERIMENTAL EVALUATION",
    "page": 7,
    "content": ""
  },
  {
    "chapter": "6.1.Experimental Setup",
    "page": 7,
    "content": "We construct a dumbbell topology shown in Figure 7 where\ntwo dummynet routers are located at the bottleneck between\ntwo end points. Each end points consists of a set of Dell\nLinux servers dedicated to high-speed TCP variant ﬂows and\nbackground traﬃc. Background traﬃc is generated by us-\ning a modiﬁcation of a web-traﬃc generator, called Surge [9]\nand Iperf [2]. We modiﬁed Surge to generate a wider range\nof ﬂow sizes in order to increase variability in cross traﬃc\nbecause medium size ﬂows tend to fully execute the slow\nstart and increase the variability in available bandwidth.\nThe RTT of each background traﬃc is randomly selected\nfrom an exponential distribution found in [7]. The socket\nbuﬀer size of background traﬃc machines is ﬁxed to de-\nfault 64KB while high-speed TCP machines are conﬁgured\nto have a very large buﬀer so that the transmission rates\nof high-speed ﬂows are only limited by the congestion con-\ntrol algorithm. Two dummynet routers and four high-speed\nTCP machines are tuned to generate or forward traﬃc close\nto 1Gbps. The details of system tuning for both Linux and\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n600\n(packets)\nTime (second)\nTCP-Friendly Region\nTCP-SACK\nCUBIC\n(a) RTT 8ms.\n0\n1000\n2000\n3000\n4000\n5000\n6000\n600\n(packets)\nTime (second)\nTCP-SACK\nCUBIC\n(b) RTT 82ms.\nﬂow. Bandwidth is set to 400Mbps.\n0\n500\n1000\n1500\n2000\n2500\n600\n(packets)\nTime (second)\nTCP-SACK flow3\nCUBIC flow4\nﬂows over 40ms RTT\nFreeBSD systems are shown in [5]. Note that Netem [22] in\nLinux provides the same functionality with the dummynet\nsoftware in FreeBSD. For this experiment, the maximum\nbandwidth of the bottleneck router is set to 400Mbps. The\nbottleneck buﬀer size is set to 100% BDP if it is not explic-\nitly speciﬁed. The amount of background traﬃc comparable\nto around 15% of the bottleneck bandwidth is pushed into\nforward and backward directions of the dumbbell. We use\nthe drop-tail router at the bottleneck."
  },
  {
    "chapter": "6.2.Intra-Protocol Fairness",
    "page": 8,
    "content": "We measure the intra-protocol fairness between two ﬂows\nof a protocol with the same RTT. We use a throughput\nratio between these two ﬂows for representing the intra-\nprotocol fairness. This metric represents a degree of band-\nwidth shares between two ﬂows of the same protocol. For\nthis experiment, we vary RTTs between 16ms and 324ms\nand test CUBIC, BIC-TCP, HSTCP, and TCP-SACK pro-\nFigure 7: Testbed\ntocols. Figure 8 (a) and 8 (b) show the intra-protocol fair-\nness and link utilization for the tested protocols. CUBIC\nand BIC-TCP show higher fairness index than TCP-SACK\nand HSTCP, representing better fair sharing between the\nﬂows. With 16ms RTT, TCP-SACK shows the best fairness\nindex indicating that Standard TCP works fairly well un-\nder small RTT networks. CUBIC, BIC-TCP, and HSTCP\nutilize the link regardless of RTTs while TCP-SACK suﬀers\nunder-utilization with larger RTTs."
  },
  {
    "chapter": "6.3.Inter-RTT Fairness",
    "page": 8,
    "content": "We measure the fairness in sharing the bottleneck band-\nwidth between two competing ﬂows that have diﬀerent RTTs.\nFor this experiment, we ﬁx RTT of one ﬂow to 162ms and\nvary RTT of the other ﬂow between 16ms and 164ms. This\nsetting gives us the RTT ratio up to 10.\nWe test CU-\nBIC, BIC-TCP, HSTCP, and TCP-SACK protocols. Fig-\nure 9 (a) shows that TCP-SACK achieves RTT fairness lin-\nearly proportional to the inverse of the RTT ratio, which\nmeans that the short RTT ﬂow has proportionally more\nbandwidth shares than the longer RTT ﬂow. Even though\nthere is no commonly accepted notion of RTT-fairness, we\nthink the proportional fairness of TCP-SACK is desirable\nbecause long RTT ﬂows tend to use more resources along\nthe longer path than short RTT ﬂows. But some of high-\nspeed protocols are desiged to provide an equal bandwidth\nsharing among the ﬂows with diﬀerent RTTs (e.g., H-TCP\nand FAST). Based on this notion of RTT fairness, if the\nRTT fairness of a protocol has a similar slope with TCP-\nSACK, we can say the protocol is “acceptable”. Figure 9\n(a) conﬁrms that CUBIC has a similar slope with TCP-\nSACK but with a higher fairness ratio indicating better\nshare of resources (bandwidth) while HSTCP fails in achiev-\ning a similar slope. Even though BIC-TCP shows the simi-\nlar slope with TCP-SACK, it shows the lowest fairness ra-\ntios among tested protocols. This is what CUBIC improves\nover BIC-TCP for RTT-fairness. We also observe that even\nthough two HSTCP ﬂows fully utilize the link regardless of\ntheir RTT ratio (See Figure 9 (b)), the slow convergence of\nHSTCP ﬂows hinders even two ﬂows of the same RTT from\nreaching to a fair share within a reasonable amount of time."
  },
  {
    "chapter": "6.4.Impact on standard TCP trafﬁc",
    "page": 8,
    "content": "As many new high-speed TCP protocols modify the win-\ndow growth function of TCP in a more scalable fashion,\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n350\nThroughput Ratio\nRTT (ms)\nTCP-SACK\nBIC-TCP\nCUBIC\n(a) Intra-Protocol Fairness.\n40\n50\n60\n70\n80\n90\n100\n350\nLink Utilization\nRTT (ms)\nTCP-SACK\nBIC-TCP\nCUBIC\n(b) Link Utilization.\nﬂows have the same RTT.\nthese new protocols tend to aﬀect the performance of Stan-\ndard TCP ﬂows which share the same bottleneck link along\nthe path. As being fair to Standard TCP is critical to the\nsafety of the protocol, we need to make sure that the window\ngrowth function of a new protocol does not unfairly aﬀect\nthe Standard TCP ﬂows.\nIn this experiment, we measure how much these high-speed\nprotocols steal the bandwidth from competing TCP-SACK\nﬂows. By following the scenarios shown in the recent eval-\nuation proposal [8], we ﬁrst measure the throughput shares\nof four TCP-SACK ﬂows when they are competing with\nthe other four TCP-SACK ﬂows.\nAfter that, we replace\nfour ﬂows with a new protocol. We measure the share of\nTCP-SACK ﬂows and the other four TCP variant ﬂows at\neach run and report only the accumulated average of their\nbandwidth shares. We test CUBIC, BIC-TCP, HSTCP, and\nTCP-SACK.\nFigure 10 (a) shows the relationship between RTT and the\nthroughput share between a new protocol ﬂows and TCP-\nSACK ﬂows. We ﬁx the bottleneck bandwidth to 400Mbps\nand vary RTT between 10ms and 160ms.\nClearly, TCP-\nSACK ﬂows do not fully utilize the bottleneck bandwidth\nas RTT increases due to its slow window growth function.\nWith 400Mbps and 160ms RTT, 8 TCP-SACK ﬂows achieve\naround 80% of the link bandwidth, but the underutilization\nwill be very serious for larger BDP path and with small num-\nber of ﬂows. CUBIC, BIC-TCP, and HSTCP fully utilize the\nlink, thanks to their scalable window growth functions. All\n0\n0.2\n0.4\n0.6\n0.8\n1\n180\nThroughput Ratio\nRTT (ms)\nTCP-SACK\nBIC-TCP\nCUBIC\n(a) Inter-RTT Fairness.\n40\n50\n60\n70\n80\n90\n100\n180\nLink Utilization\nRTT (ms)\nTCP-SACK\nBIC-TCP\nCUBIC\n(b) Link Utilization.\nﬂow varies its RTT from 16ms to 162ms.\nthe tested high-speed protocols grab more bandwidth share\nfrom TCP-SACK as RTT increases. Also we conﬁrm that\nCUBIC gives more room to TCP-SACK than BIC-TCP and\nHSTCP for whole range of tested RTTs while achieving full\nutilization of the path. This is one of the design objective of\nCUBIC that it operates like BIC-TCP and be nice to other\nﬂows in the network. As RTT increases, CUBIC, BIC-TCP\nand HSTCP steal more bandwidth from TCP-SACK. Some\namount of bandwidth they steal is from the amount of band-\nwidth that TCP-SACK doesn’t utilize.\nFigure 10 (b) and 10 (c) show the performance results re-\ngarding the TCP friendliness over short-RTT networks (10ms\nRTT) and long-RTT networks (100ms RTT), respectively.\nAccording to [15], under high loss rate regions (small-RTT\nnetworks) where TCP is well-behaving, the protocol must\nbehave like TCP, and under low loss rate regions (large-\nRTT networks) where TCP has a low utilization problem,\nit can use more bandwidth than TCP. As shown in Fig-\nure 10 (b), with 10ms RTT, we can see that TCP-SACK\nstill uses the full bandwidth. In this region, all high-speed\nprotocols need to be friendly to TCP-SACK by following the\narguments above. Interestingly, CUBIC behaves more TCP-\nfriendly even comparing to TCP-SACK for certain band-\nwidths.\nRather than stealing the bandwidth from TCP-\nSACK ﬂows, CUBIC ﬂows employs a window growth func-\ntion that is comparable to TCP-SACK, so that competing\nTCP-SACK ﬂows have the same chance with CUBIC ﬂows\nfor grabbing the bandwidth shares. However, BIC-TCP and\nHSTCP show a tendency to operate in a scalable mode\n(being more aggressive) as the link speed increases. Even\nthough the graph doesn’t show the results corresponding to\nthe link speed beyond 400Mbps, it is obvious that a scalable\nmode of BIC-TCP and HSTCP will deprive most of band-\nwidth share of TCP-SACK. As most high-speed TCP pro-\ntocols including BIC-TCP and HSTCP achieve TCP friend-\nliness by having some form of “TCP modes” during which\nthey behave in the same way as TCP. BIC-TCP and HSTCP\nenter their TCP mode when the window size is less than\n14 and 38 packets, respectively. Therefore, even with 1ms\nRTT, if BDP is larger than 38 packets, HSTCP will operate\nin a scalable mode. This is the limitation when the protocol\nuses a ﬁxed cutoﬀfor detecting a TCP-friendly region. CU-\nBIC deﬁnes a TCP-friendly region in real-time; therefore,\nCUBIC doesn’t have this scalability problem.\nFigure 10 (c) shows the results with 100ms RTT. All four\nprotocols show reasonable friendliness to TCP. As the band-\nwidth gets larger than 10Mbps, the throughput ratio drops\nquite rapidly. As CUBIC, like BIC-TCP and HSTCP, re-\ngards this operating region is out of TCP-friendly region\nand behaves to be scalable to this environment.\nCUBIC\nand BIC-TCP show a similar aggressiveness which is slightly\nmore aggressive† than HSTCP especially for the bandwidth\nless than 100Mbps.\nThrough an extensive testing [4], we\nconﬁrm that this doesn’t highly impact on the performance\nof TCP-SACK."
  },
  {
    "chapter": "7.CONCLUSION",
    "page": 10,
    "content": "We propose a new TCP variant, called CUBIC, for fast\nand long distance networks.\nCUBIC is an enhanced ver-\nsion of BIC-TCP. It simpliﬁes the BIC-TCP window control\nand improves its TCP-friendliness and RTT-fairness. CU-\nBIC uses a cubic increase function in terms of the elapsed\ntime since the last loss event. In order to provide fairness\nto Standard TCP, CUBIC also behaves like Standard TCP\nwhen the cubic window growth function is slower than Stan-\ndard TCP. Furthermore, the real-time nature of the pro-\ntocol keeps the window growth rate independent of RTT,\nwhich keeps the protocol TCP friendly under both short\nand long RTT paths. We show the details of Linux CUBIC\nalgorithm and implementation. Through extensive testing,\nwe conﬁrm that CUBIC tackles the shortcomings of BIC-\nTCP and achieves fairly good Intra-protocol fairness, RTT-\nfairness and TCP-friendliness."
  },
  {
    "chapter": "8.REFERENCES",
    "page": 10,
    "content": "2.6.git;a=history;f=net/ipv4/tcp\n†We used the latest update of CUBIC (v2.2) which improved\nthe scalability and convergence speed of the protocol, which\ndoesn’t clamp the increment in both convex and cocave re-\ngions. A slight increase of aggressivenss is the trade-oﬀbe-\ntween scalability and TCP-friendlines. Our extensive testing\nconﬁrms that CUBIC has better scalability and convergence\nspeed with this small change (trade-oﬀ) while obtaining rea-\nsonable TCP-friendliness.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n10\nThroughput (Mbps)\nRTT (ms)\nBIC-TCP\nCUBIC\n(a) Bandwidth 400Mbps with varying RTT\n0\n20\n40\n60\n80\n100\n120\n10\nLink Utilization (%)\nLink Speed (Mbps)\nBIC-TCP\nCUBIC\n(b) RTT 10ms with varying bandwidth\n0\n20\n40\n60\n80\n100\n120\n10\nLink Utilization (%)\nLink Speed (Mbps)\nBIC-TCP\nCUBIC\n(c) RTT 100ms with varying bandwidth\nFigure 10: Impact on standard TCP traﬃc.\nTesting\nWorkshop (November 2001).\nThe authors releases the ﬁrst CUBIC\nimplementation in Linux to the Linux\ncommunity [18].\nCUBIC is oﬃcially included in the\nLinux kernel.\nCUBIC replaces BIC-TCP as the de-\nfault TCP protocol in Linux kernel.\nThe original implementation of CUBIC\nhas a scaling bug. It has taken about\na month to ﬁx this bug since CUBIC\nreplaced BIC-TCP.\nIts original implementation by the au-\nthors are optimized by the Linux de-\nveloper for better performance [20, 29].\nIn particular, the cubic root calculation\nin CUBIC, originally implemented in\nthe bisection method, is now replaced\nby a Newton-Raphson method with ta-\nble loopups for small values. This re-\nsults in more than 10 times perfor-\nmance improvement in the cubic root\ncalculation. On average, the bisection\nmethod costs 1032 clocks while the im-\nproved version costs only 79 clocks.\nThe original implementation of CU-\nBIC clamped the maximum window in-\ncrement to 32 packets per RTT. This\nfeature is inherited from\nBIC-TCP\n(Smax). An extensive lab testing con-\nﬁrmed that CUBIC can safely remove\nthis window clamping in the concave\nregion. This enhances the scalability of\nCUBIC over very large BDP network\npaths. This is incorporated in CUBIC\n2.1 (Linux 2.6.22).\nCUBIC improves slow start for fast\nstart-up by removing initial\nThe use of received timestamp op-\ntion value from RTT calculation is\nremoved for preventing possible ma-\nlicious receiver attacks that reports\nwrong timestamps to reduce RTTs for\nmore throughput.\nThe window clamping during the con-\nvex growth phase is also removed. This\nfeature allows CUBIC to improve its\nconvergence speed while maintaining\nits fairness and TCP friendliness.\nTable 1: CUBIC version history"
  }
]
